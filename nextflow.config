/*
 * -------------------------------------------------
 *  UCT dada2 Nextflow config file
 * -------------------------------------------------
 * Default config options for all environments.
 * Cluster-specific config options should be saved
 * in the conf folder and imported under a profile
 * name here.
 */

// Global default params, used in configs
// Some help with time stamps
import java.text.SimpleDateFormat
params {
  name = false
  version = 0.4 //pipeline version
  // Pipeline Options
  reads = "/data/runs/4000-samples_dada2/run1/*_R{1,2}.fastq"
  timestamp = new SimpleDateFormat("yyyy-MM-dd").format(new java.util.Date())
  outdir = "/data/runs/4000-samples_dada2/output_all/" + timestamp + "-dada2"
  ticket = 0 //for Redmine, not currently used (KL)
  
  // Trimming
  trimFor = 0 
  trimRev = 0
  truncFor = 248
  truncRev = 212
  maxEEFor = 2
  maxEERev = 2
  truncQ = 2 //default
  maxN = 0 //default
  maxLen = 'Inf' //default
  minLen = 50 //default
  rmPhiX = "T"


  // Merging
  minOverlap = 20 //default=12
  maxMismatch = 0 //default
  trimOverhang = "F" // KL: I don't think we have overhangs for WISH project03

  reference = "/data/runs/4000-samples_dada2/silva132_ref/silva_nr_v132_train_set.fa"
  
  species = "/data/runs/4000-samples_dada2/silva132_ref/silva_species_assignment_v132.fa"

  // NYI, for dada sample inference pooling (requires all samples)
  pool = "pseudo"
}

profiles {

  uct_hex{
    includeConfig 'conf/uct_hex.config'
    includeConfig 'conf/base.config'
  }
  uct_hpc{
    includeConfig 'conf/icts_hpc.config'
    includeConfig 'conf/base.config'
  }
  ilifu{
    includeConfig 'conf/ilifu.config'
    includeConfig 'conf/base.config'
   } 
  icts_hpc{
    includeConfig 'conf/icts_hpc.config'
    includeConfig 'conf/base.config'
  }
  azure{
    includeConfig 'conf/base.azure.config'
    includeConfig 'conf/azure.config'
  }
  none {
    // Don't load any config (for use with custom home configs)
  }
 
}


// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

timeline {
  enabled = true
  file = "${params.outdir}/pipeline_info/dada2_timeline.html"
}
report {
  enabled = true
  file = "${params.outdir}/pipeline_info/dada2_report.html"
}
trace {
  enabled = true
  file = "${params.outdir}/pipeline_info/dada2_trace.txt"
}
dag {
  enabled = true
  file = "${params.outdir}/pipeline_info/dada2_DAG.svg"
}

manifest {
  homePage = 'https://github.com/kviljoen/16S-rDNA-dada2-pipeline'
  description = 'Nextflow dada2 analysis pipeline for UCT CBIO'
  mainScript = 'main.nf'
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
  if(type == 'memory'){
    try {
      if(obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if(type == 'time'){
    try {
      if(obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if(type == 'cpus'){
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}
